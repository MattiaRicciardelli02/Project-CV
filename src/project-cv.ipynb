{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7442,"sourceType":"datasetVersion","datasetId":4789},{"sourceId":2271054,"sourceType":"datasetVersion","datasetId":76785},{"sourceId":4961490,"sourceType":"datasetVersion","datasetId":2877506}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Project 9: Advanced Out-of-Distribution Detection for Multi-Class Classification**   ","metadata":{}},{"cell_type":"markdown","source":"## Project Overview\n\nThe goal of this project is to address the challenge of out-of-distribution (OOD) detection in deep learning, particularly in the context of multi-class image classification. In real-world scenarios, models often encounter data that differ significantly from their training distribution, which can lead to unreliable or unsafe predictions. Therefore, detecting and properly handling OOD inputs is crucial for building robust and trustworthy AI systems.\n\nThe first step involves training a deep neural network classifier on the **Food-101** dataset, which serves as the in-distribution (ID) data. This dataset includes high-dimensional images with complex visual features, making it a suitable benchmark for evaluating model performance in realistic settings.\n\nOnce the classifier is trained, we will implement one or more OOD detection methods. The **SVHN (Street View House Numbers)** dataset will be used as the primary OOD source, providing visually distinct samples that test the model’s ability to separate known from unknown inputs. Techniques such as energy-based or gradient-based scoring can be applied to detect OOD samples effectively.\n\nAdditionally, the project can be extended by incorporating other OOD datasets to further assess the generalizability of the detection framework. The final objective is to evaluate the model’s capability to distinguish between in-distribution and out-of-distribution examples using appropriate metrics, such as AUROC, AUPR, and FPR@95.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"markdown","source":"In this section, we import all the necessary libraries required for building, training, and evaluating the model, as well as for implementing OOD detection. These include:\n\n- **Torch / torchvision** for model definition, training, and dataset loading\n- **NumPy / Pandas** for data manipulation\n- **Matplotlib** for visualization\n- **Scikit-learn** for computing evaluation metrics\n- **Custom utility functions** (if any) for training loops, loss functions, and OOD scoring","metadata":{}},{"cell_type":"code","source":"!pip install tqdm\n\n# Core\nimport os\nimport time\nimport copy\nimport random\nfrom collections import defaultdict\nfrom itertools import cycle, islice\n\n\nimport numpy as np\nfrom scipy.io import loadmat\nfrom PIL import Image\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset, ConcatDataset, Dataset, random_split\nfrom torch.optim.lr_scheduler import (\n    ReduceLROnPlateau, StepLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\n)\n\n# torchvision\nfrom torchvision import datasets, transforms, models\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.datasets import ImageFolder, VisionDataset\n\n# Progress bar\nfrom tqdm.notebook import tqdm\n\n# Scikit-learn\nfrom sklearn.metrics import (\n    roc_auc_score, roc_curve, precision_recall_curve, auc,\n    confusion_matrix, average_precision_score\n)\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hardware Check\n\nBefore starting the training process, we check whether a GPU is available for computation. If CUDA is available, the code will use the GPU to accelerate training. Otherwise, it will fall back to the CPU. Basic system information, such as GPU model and memory or CPU details, is also displayed to help monitor the hardware configuration.","metadata":{}},{"cell_type":"code","source":"def hardware_check():\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(f\"GPU is available!\")\n        print(f\"  -> GPU - {torch.cuda.get_device_name()}\")\n        print(f\"  -> Total Memory: {torch.cuda.get_device_properties().total_memory / 1024**3:.2f} GB\")\n    else:\n        device = torch.device(\"cpu\")\n        print(\"GPU is not available, using CPU.\")\n        print(\"\\nCPU Information:\")\n        cpu_model = os.popen(\"cat /proc/cpuinfo | grep \\\"model name\\\" | uniq\").read().strip()\n        print(f\"CPU Model: {cpu_model}\")\n        print(f\"Number of CPU cores: {os.cpu_count()}\")\n    return device\n\ndevice = hardware_check()\nprint(f\"\\nUsing {device} for computation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Globals\n\nWe use a batch size of 64 throughout, and two separate initial learning rates:\n* LR_ID_INIT for the base Food-101 fine-tuning phase\n* LR_OE_INIT for the Outlier Exposure stage\n\nA small weight decay (WEIGHT_DECAY=1e-4) helps regularize training in both phases. Each phase runs for 15 epochs (EPOCHS_ID / EPOCHS_OE), with early stopping after 6 epochs of no improvement during OE (PATIENCE=6).\n\nFor Outlier Exposure specifically, we introduce:\n* energy margins M_IN / M_OUT to define hinge losses on in- and out-of-distribution energies\n* a maximum OE weight LAMBDA_MAX and balance term ALPHA_ID\n* a warm-up (3 epochs) and linear ramp (2 epochs) schedule to gradually apply the OE penalty\n* a softmax temperature TEMPERATURE for energy scoring\n\nAll images are normalized with the standard ImageNet channel means and standard deviations (MEAN, STD) for consistency with the pretrained backbone.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE      = 64\nLR_ID_INIT      = 2e-4      # 1° training\nLR_OE_INIT      = 3e-4      # 2° training\nWEIGHT_DECAY    = 1e-4\nEPOCHS_ID       = 15\nEPOCHS_OE       = 15 \n\n# Parameters for OE training\nM_IN, M_OUT     = -6.0, -0.5\nLAMBDA_MAX      = 0.06\nALPHA_ID        = 0.05\nWARMUP_EP       = 3\nRAMP_EP         = 2\nPATIENCE        = 7\nTEMPERATURE     = 1.0\n\nMEAN = [0.485, 0.456, 0.406]\nSTD  = [0.229, 0.224, 0.225]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data and Trasformation\n\nBelow we define the image‐level preprocessing pipelines for our in-distribution (ID) Food-101 data and out-of-distribution (OOD) SVHN data. During training we apply a rich suite of random augmentations—resizing, random crops, flips, rotations, color jitter, affine and perspective warps, plus random erasing—to help the model learn robust, invariant features. At test time (both for ID and OOD) we use a single deterministic resize, center crop and normalize flow so that evaluation is stable and reproducible.","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random')\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD)\n])\n\nood_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN,STD)\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Food-101**\n\nWe load Food-101 via ImageFolder, pointing it at the top-level image directories, and then apply the official train.txt / test.txt metadata files to split exactly as the dataset creators intended. From the resulting training set, we carve off a stratified 10% sample, meaning we randomly select examples in proportion to each of the 101 classes, to serve as our validation set. This simple hold-out preserves the original class balance, so we can tune hyperparameters and monitor for overfitting without disturbing the integrity of the published train/test split.","metadata":{}},{"cell_type":"code","source":"val_split = 0.1  # 10% per validation\n\n# Import from Kaggle\ndataset_root = \"/kaggle/input/food101/food-101\"\nimages_root = os.path.join(dataset_root, \"images\")\ntrain_txt = os.path.join(dataset_root, \"meta\", \"train.txt\")\ntest_txt = os.path.join(dataset_root, \"meta\", \"test.txt\")\n\nfull_dataset_train = ImageFolder(root=images_root, transform=train_transform)\nfull_dataset_eval  = ImageFolder(root=images_root, transform=test_transform)\n\nprint(f\"Complete dataset loaded: {len(full_dataset_train)} total images\")\nprint(f\"Number of classes: {len(full_dataset_train.classes)}\")\n\n# Split in train and test\ndef load_indices(txt_file, dataset):\n    with open(txt_file, \"r\") as f:\n        lines = f.read().splitlines()\n    idxs = []\n    for rel_path in lines:\n        for i, (img_path, _) in enumerate(dataset.samples):\n            if rel_path in img_path:\n                idxs.append(i)\n                break\n    return idxs\n\ntrain_indices = load_indices(train_txt, full_dataset_train)\ntest_indices = load_indices(test_txt, full_dataset_train)\n\n# Balanced Validation Set \ntrain_labels = [full_dataset_train.samples[i][1] for i in train_indices]\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=val_split, random_state=42)\ntrain_idx, val_idx = next(splitter.split(train_indices, train_labels))\n\ntrain_id = Subset(full_dataset_train, [train_indices[i] for i in train_idx])\nval_id   = Subset(full_dataset_eval,  [train_indices[i] for i in val_idx])\ntest_id  = Subset(full_dataset_eval,  test_indices)\n\nprint(f\"Train: {len(train_id)}, Validation: {len(val_id)}, Test: {len(test_id)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SVHN**\n\nFor Outlier Exposure, we can’t simply use ImageFolder because SVHN is stored in MATLAB .mat files.  So we wrap those with a tiny VisionDataset subclass that (a) loads the .mat, (b) transposes the arrays into standard H×W×C form, and then (c) returns only images, no labels, so that our OE loss sees “unlabeled” outliers.  From the SVHN training pool we then draw a random 30% sample (via torch.randperm) to use during model fitting, while reserving the full SVHN test set as a clean benchmark for OOD detection after training.","metadata":{}},{"cell_type":"code","source":"# Return only img (without label)\nclass SVHNFromMat(VisionDataset):\n    def __init__(self, mat_path, transform=None):\n        super().__init__(root=\"\", transform=transform)\n        data = loadmat(mat_path)\n        self.images = np.transpose(data['X'], (3, 0, 1, 2))  \n\n    def __getitem__(self, index):\n        img = Image.fromarray(self.images[index])\n        if self.transform:\n            img = self.transform(img)\n        return img  # 🔁 SOLO immagine, nessuna label\n\n    def __len__(self):\n        return len(self.images)\n\n# Import from Kaggle\nsvhn_train_path = \"/kaggle/input/svhndataset/train_32x32.mat\"\nsvhn_test_path = \"/kaggle/input/svhndataset/test_32x32.mat\"\n\nsvhn_train = SVHNFromMat(svhn_train_path, transform=ood_transform)\nsvhn_test = SVHNFromMat(svhn_test_path, transform=ood_transform)\n\n# Subset for training OOD (30%)\nood_fraction = 0.30\nsubset_idx   = torch.randperm(len(svhn_train))[: int(ood_fraction * len(svhn_train))]\nsvhn_subset  = Subset(svhn_train, subset_idx)\n\nprint(f\"SVHN subset created: {len(svhn_subset)} images \"\n      f\"({ood_fraction*100:.0f}% di {len(svhn_train)})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After loading the datasets, we wrap each split in a PyTorch DataLoader to efficiently iterate over mini-batches during training and evaluation. We shuffle only the training streams and keep validation/test loaders deterministic. We also halve the OOD batch size so that mixing ID+OOD examples still yields a balanced overall batch.","metadata":{}},{"cell_type":"code","source":"train_id_loader = DataLoader(train_id , batch_size=BATCH_SIZE, shuffle=True ,\n                             num_workers=2)\nval_id_loader   = DataLoader(val_id   , batch_size=BATCH_SIZE, shuffle=False,\n                             num_workers=2)\nsvhn_loader     = DataLoader(svhn_subset, batch_size=BATCH_SIZE//2, shuffle=True,\n                             num_workers=2)\ntest_loader     = DataLoader(test_id, batch_size=BATCH_SIZE, shuffle=False, \n                             num_workers=2)\nood_loader      = DataLoader(svhn_test, batch_size=BATCH_SIZE, shuffle=False, \n                             num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Oxford 102 Flower Dataset**\n\nTo further evaluate the model's ability to distinguish in-distribution (Food-101) from out-of-distribution (OOD) samples, we use the Flower Dataset as an additional OOD source.\n\nThis dataset consists of flower images from various categories and lies outside the food domain, making it a suitable benchmark to test the generalization of OOD detection.\n\n","metadata":{}},{"cell_type":"code","source":"flower_ood_dataset = ImageFolder(\n    root='/kaggle/input/pytorch-challange-flower-dataset/dataset/train',\n    transform=ood_transform\n)\n\nflower_ood_loader = DataLoader(flower_ood_dataset, batch_size=64, shuffle=False, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Network\n\nAs the starting point for our project, we chose ResNet-50 as our model because it offers a great trade-off between accuracy and speed. Its residual blocks let us go deep enough to learn complex visual patterns (textures, shapes, colors) without training instabilities, and starting from ImageNet–pretrained weights means we need fewer epochs and less data to reach strong performance on 101 food categories.\n\n**How `build_resnet50_food101` works**  \n1. **Load the backbone**  \n   We instantiate the standard ResNet-50 model, optionally loading ImageNet weights to give us rich, general-purpose feature detectors from the very first iteration.  \n2. **Swap in a new head**  \n   The original 1000-class head is replaced with a small block that first applies dropout (to prevent overfitting) and then a linear layer that outputs exactly 101 logits—one per food category.  \n\nBy wrapping these steps in a single function, we guarantee that both our baseline and Outlier Exposure experiments use the exact same architecture and initialization.  ","metadata":{}},{"cell_type":"code","source":"def build_resnet50_food101(\n        num_classes: int = 101,\n        pretrained: bool = True,\n        dropout_p: float = 0.3,\n        device: torch.device | str = None\n) -> nn.Module:\n    \n    weights = ResNet50_Weights.DEFAULT if pretrained else None\n    model = resnet50(weights=weights)\n\n    model.fc = nn.Sequential(\n        nn.Dropout(dropout_p),\n        nn.Linear(model.fc.in_features, num_classes)\n    )\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Standard In-Distribution Training\n\nWe begin by training a standard ResNet-50 model on Food-101 only, no out-of-distribution data, no auxiliary losses.  Our objectives are:  \n1. Teach the network to discriminate among the 101 food categories as effectively as possible.  \n2. Produce a “vanilla” classifier that we can later probe on truly out-of-distribution images (SVHN and beyond) to quantify its native OOD rejection behavior.\n\nDuring each epoch we:\n\n1. **Forward + Backward**  \n   For each mini-batch, we compute the network logits and convert them to probabilities via softmax:  \n   $$\n   p_{i,c} \\;=\\; \\frac{\\exp(z_{i,c})}{\\sum_{k=1}^{101}\\exp(z_{i,k})}\\,.\n   $$  \n   We then apply label smoothing (\\$\\epsilon=0.1\\$) to the true-class probability:  \n   $$\n   p_{i,y_i}^* \\;=\\; (1 - \\epsilon)\\times 1 \\;+\\; \\frac{\\epsilon}{101}\\,.\n   $$  \n   The resulting cross-entropy loss is  \n   $$\n   \\mathcal{L}_{\\mathrm{CE}}\n   \\;=\\;\n   -\\frac{1}{B}\\sum_{i=1}^{B} \\log p_{i,y_i}^*\\,.\n   $$  \n    Calling `loss.backward()` back-propagates the gradient  \n    $$\\nabla \\mathcal{L}_{\\mathrm{CE}}$$  \n    through the network, and `optimizer.step()` (Adam) updates every parameter accordingly. This pair of operations—forward pass to compute $$\\mathcal{L}_{\\mathrm{CE}}$$  \n    then backward pass to adjust parameters—drives the model to better classify each food category.  \n2. **Learning-rate schedule**  \n   Every 5 epochs we halve the learning rate. This lets the optimizer make big updates early, then finer adjustments later.\n\n3. **Progress monitoring**  \n   A `tqdm` progress bar shows the running loss and top-1 accuracy for each mini-batch.\n\nThis run produces our baseline Food-101 classifier. In the next phase we’ll evaluate its OOD detection performance (energy and MSP scores on SVHN) before adding any Outlier Exposure penalties.  ","metadata":{}},{"cell_type":"code","source":"model = build_resnet50_food101()    \nmodel = model.to(device)\n\ncriterion  = nn.CrossEntropyLoss(label_smoothing=0.1)  \n\noptimizer  = optim.Adam(model.parameters(),lr=LR_ID_INIT, weight_decay=WEIGHT_DECAY)\n\n\n# Training Loop\n\nprint(f\"\\n Initial Learning-rate: {LR_ID_INIT:.1e}\")\n\nfor epoch in range(EPOCHS_ID):                   \n    # scheduler: halves the LR every 5 epochs\n    if epoch > 0 and epoch % 5 == 0:    \n        for g in optimizer.param_groups:\n            g[\"lr\"] *= 0.5            \n        print(f\"\\n LR halved : {optimizer.param_groups[0]['lr']:.1e}\")\n\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS_ID} — samples: {len(train_id_loader.dataset)}\")\n    model.train()\n    running_loss = 0.0\n    correct = total = 0\n\n    loop = tqdm(train_id_loader, leave=False)\n    for images, labels in loop:\n        images, labels = images.to(device), labels.to(device)\n\n        # forward + backward\n        outputs = model(images)\n        loss    = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        total   += labels.size(0)\n        correct += (outputs.argmax(1) == labels).sum().item()\n\n        loop.set_postfix(\n            loss = running_loss / (loop.n + 1),\n            acc  = 100.0 * correct / total\n        )\n\n    print(f\" Epoch {epoch+1} — Train ACC: {100.0 * correct / total:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation\n\nIn the evaluation phase, we first measure the classifier’s accuracy on in-distribution (Food-101) test images to quantify standard recognition performance. Next, we assess the model’s ability to detect out-of-distribution (OOD) samples by feeding it SVHN images and computing energy and max-softmax scores. From these scores we derive:\n\t\n* AUROC to capture overall separability between ID and OOD.\n* FPR@95 TPR to report the false positive rate when correctly identifying 95 % of in-distribution samples.\n* AUPR-In to summarize precision-recall trade-offs treating ID as the positive class.\n\n\nPlotting ROC and PR curves alongside score histograms completes the evaluation, giving both quantitative metrics and visual insight into how well the model distinguishes familiar from unfamiliar images.","metadata":{}},{"cell_type":"code","source":"model.eval()\nT = 0.2                          # Temperature found with grid-search\n\ncorrect = 0\ntotal = 0\n\n# Accuracy\n\nwith torch.no_grad():\n    test_loop = tqdm(test_loader, total=len(test_loader), desc=\"Testing\")\n    for images, labels in test_loop:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint(f\"\\nFinal Test Accuracy: {100 * correct / total:.2f}%\")\n\n# Useful functions\n\ndef fpr_at_given_tpr(fpr, tpr, target_tpr=0.95):\n    if tpr[-1] < target_tpr:     \n        return 1.0\n    return float(np.interp(target_tpr, tpr, fpr))\n\ndef _images_from(batch):\n    return batch[0] if isinstance(batch, (list, tuple)) else batch\n\n\n# Scoring Functions\n\ndef energy_scores(loader, T):\n    out = []\n    for batch in loader:\n        x = _images_from(batch).to(device, non_blocking=True)\n        out.append((-torch.logsumexp(model(x) / T, 1)).cpu())\n    return torch.cat(out).numpy()        # shape (N,)\n\n@torch.no_grad()\ndef msp_scores(loader):\n    out = []\n    for batch in tqdm(loader, desc=\"MSP\", leave=False):\n        x = _images_from(batch).to(device, non_blocking=True)\n        out.append(torch.softmax(model(x), dim=1).max(1).values.cpu())\n    return torch.cat(out).numpy()\n\nid_energy  = energy_scores(test_loader, T)\nood_energy = energy_scores(ood_loader,  T)\n\nid_msp  = msp_scores(test_loader)\nood_msp = msp_scores(ood_loader)\n\n\n# m-shift sull’Energy  \n\nm            = np.percentile(id_energy, 95)\nid_energy_s  = id_energy  - m\nood_energy_s = ood_energy - m\nenergy_cat   = -np.concatenate([id_energy_s, ood_energy_s])   # alto = ID\nlabels       = np.concatenate([np.ones_like(id_energy_s),\n                               np.zeros_like(ood_energy_s)])\n\n\n# AUROC / AUPR  \nauroc_energy = roc_auc_score(labels, energy_cat)\nauroc_msp    = roc_auc_score(labels, np.concatenate([id_msp, ood_msp]))\n\nfpr_e, tpr_e, _ = roc_curve(labels, energy_cat)\nfpr_s, tpr_s, _ = roc_curve(labels, np.concatenate([id_msp, ood_msp]))\n\n# False Positive Rate\ntau_energy   = 0.0                     \ntau_msp      = np.percentile(id_msp, 5)\n\nfpr95_energy = (ood_energy_s >  tau_energy).mean()\nfpr95_msp    = (ood_msp      >  tau_msp   ).mean()\n\npr_e, rc_e, _ = precision_recall_curve(labels, energy_cat)\npr_s, rc_s, _ = precision_recall_curve(labels,\n                                       np.concatenate([id_msp, ood_msp]))\naupr_e = average_precision_score(labels, energy_cat)\naupr_m = average_precision_score(labels, np.concatenate([id_msp, ood_msp]))\n\n# Results\n\nprint(\"\\n METRICHE OOD \")\nprint(f\"• AUROC  (Energy)      : {auroc_energy:.4f}\")\nprint(f\"• AUROC  (Soft-max)    : {auroc_msp:.4f}\")\nprint(f\"• FPR@95TPR (Energy)   : {fpr95_energy*100:.2f}%\")\nprint(f\"• FPR@95TPR (Soft-max) : {fpr95_msp*100:.2f}%\")\nprint(f\"• AUPR-In (Energy)     : {aupr_e:.4f}\")\nprint(f\"• AUPR-In (Soft-max)   : {aupr_m:.4f}\")\n\n\n# Curve ROC / PR\n\nplt.figure(figsize=(14, 5))\n# ROC\nplt.subplot(1, 2, 1)\nplt.plot(fpr_e, tpr_e, label=f\"Energy (AUROC={auroc_energy:.4f})\")\nplt.plot(fpr_s, tpr_s, label=f\"MSP   (AUROC={auroc_msp:.4f})\")\nplt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC — OOD Detection\"); plt.legend(); plt.grid(True)\n\n# PR\nplt.subplot(1, 2, 2)\nplt.plot(rc_e, pr_e, label=f\"Energy (AUPR={aupr_e:.4f})\")\nplt.plot(rc_s, pr_s, label=f\"MSP   (AUPR={aupr_m:.4f})\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall (ID positive)\"); plt.legend(); plt.grid(True)\nplt.tight_layout(); plt.show()\n\n# Istogrammi\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# Energy\naxes[0].hist(id_energy_s,  bins=100, alpha=0.6, label=\"ID (Food101)\")\naxes[0].hist(ood_energy_s, bins=100, alpha=0.6, label=\"OOD (SVHN)\")\naxes[0].axvline(0, color=\"k\", ls=\"--\")\naxes[0].set_title(\"Distribuzione Energy (shift)\")\naxes[0].set_xlabel(\"Energy shiftato  (alto = ID)\")\naxes[0].legend()\n\n# MSP\naxes[1].hist(id_msp,  bins=100, alpha=0.6, label=\"ID (Food101)\")\naxes[1].hist(ood_msp, bins=100, alpha=0.6, label=\"OOD (SVHN)\")\naxes[1].axvline(tau_msp, color=\"k\", ls=\"--\")\naxes[1].set_title(\"Distribuzione Max-Softmax\")\naxes[1].set_xlabel(\"Max Softmax  (alto = ID)\")\naxes[1].legend()\n\nplt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Outlier Exposure: Training ID + OOD \nIn this phase, we augment the standard Food-101 fine-tuning with SVHN images and an auxiliary energy-based penalty to push OOD samples away from the in-distribution manifold.  The network still learns the 101 food classes, but now also learns to assign higher “energy” to outliers.\n\nDuring each epoch:\n\n1. **Outlier Exposure Loss**\n\n    We process mixed mini-batches of ID and OOD images.  For ImageNet-pretrained ResNet-50 with a 101-way head, let\n    $$\n    E(\\mathbf{z}) = -T,\\log\\sum_c\\exp\\bigl(z_c/T\\bigr)\n    $$\n    be the energy of logits $\\mathbf{z}$.  We impose two hinge-style penalties:\n    $$\n    \\mathcal{L}_{\\text{ID}}  = \\bigl[\\max(E_{\\text{ID}} - m_{\\text{in}},0)\\bigr]^2,\\quad\n    \\mathcal{L}_{\\text{OOD}} = \\bigl[\\max(m_{\\text{out}} - E_{\\text{OOD}},0)\\bigr]^2,\n    $$\n    and combine them into\n    $$\n    \\mathcal{L}_{\\text{OE}} = \\lambda*\\bigl(\\mathcal{L}_{\\text{OOD}} + \\alpha*\\mathcal{L}_{\\text{ID}}\\bigr).\n    $$\n    The full training objective is\n    $$\n    \\mathcal{L} = \\mathcal{L}_{\\text{CE}} + \\mathcal{L}_{\\text{OE}}.\n    $$\n    After computing the combined loss, we perform back-propagation of its gradient through the network and then update every model parameter via the optimizer’s update rule.\n    \t\n2. **Warm-up & Ramp for OE Strength**\n\n\n   We gradually increase $\\lambda$ over the first few epochs (warm-up+ramp), then hold it constant at its maximum value.  This avoids overwhelming the classifier before it has learned good food-class features.\n\t\n4. **Label-smoothing Schedule**\n\n\n   After the OE ramp completes, we decay the label-smoothing factor by 0.05 each epoch to sharpen the ID classification head over time.\n\t\n6.\t**Cosine-Restarts Scheduler**\n\n\n  We use CosineAnnealingWarmRestarts to briefly boost the learning rate at regular intervals (T₀=5, Tₘₚₗ=2), enabling fresh exploration without extending total epochs.\n\t\n8.\t**Monitoring & Early-Stop**\n\n\n  A tqdm bar tracks batch-level CE and OE losses plus training accuracy.  Validation loss triggers best-model snapshots, and training stops automatically if no improvement occurs for PATIENCE epochs.\n\n10.\t**Batch-Norm re-calibration**\n\n   Once we’ve picked the best model, we feed all Food-101 training images through it one last time (still in training mode but without changing any weights). This lets each BatchNorm layer recompute its running averages so they match the true data—making the model’s behavior more reliable when we finally test it.\n\nThis procedure teaches the model not only to recognise food categories, but also to push SVHN samples into a high-energy “outlier” regime, laying the groundwork for robust OOD detection.","metadata":{}},{"cell_type":"code","source":"model = build_resnet50_food101()    \nmodel = model.to(device)\n\noptimizer  = optim.Adam(model.parameters(), lr=LR_OE_INIT,weight_decay=WEIGHT_DECAY)\n\nscheduler = CosineAnnealingWarmRestarts(optimizer,T_0=5, T_mult=2, eta_min=1e-6)\n\nlabel_smooth = 0.1                     \ncriterion_id = nn.CrossEntropyLoss(label_smoothing=label_smooth)\ncriterion_val = nn.CrossEntropyLoss()\n\n\ndef energy(logits, T=TEMPERATURE):\n    return -T * torch.logsumexp(logits / T, dim=1)\n\n\n# Training loop\n\nbest_val_loss = float(\"inf\")\nbest_wts      = copy.deepcopy(model.state_dict())\nno_improve    = 0\n\nfor epoch in range(EPOCHS_OE):\n\n    # warm-up + ramp for lambda\n    if   epoch < WARMUP_EP:\n        lam = 0.0\n    elif epoch < WARMUP_EP + RAMP_EP:\n        lam = LAMBDA_MAX * (epoch - WARMUP_EP + 1) / RAMP_EP\n    else:\n        lam = LAMBDA_MAX\n\n    # progressive label-smoothing \n    if epoch >= WARMUP_EP + RAMP_EP and label_smooth > 0:\n        label_smooth = max(0.0, label_smooth - 0.05)\n        criterion_id.label_smoothing = label_smooth\n\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS_OE}  |  λ={lam:.3f}  |  \"\n          f\"LR={optimizer.param_groups[0]['lr']:.2e}  |  LS={label_smooth:.2f}\")\n\n    model.train()\n    ood_iter = iter(svhn_loader)\n    run_gap = correct = total = 0\n\n    train_loop = tqdm(\n        train_id_loader,\n        total=len(train_id_loader),\n        desc=f\"Train {epoch+1}\",\n        leave=False\n    )\n\n    for i, (x_id, y_id) in enumerate(train_loop, 1):\n        try:\n            x_ood = next(ood_iter)\n        except StopIteration:\n            ood_iter = iter(svhn_loader)\n            x_ood    = next(ood_iter)\n\n        x_id, y_id, x_ood = x_id.to(device), y_id.to(device), x_ood.to(device)\n\n        # forward + losses \n        logits_id  = model(x_id)\n        logits_ood = model(x_ood)\n        loss_ce    = criterion_id(logits_id, y_id)\n\n        e_id    = energy(logits_id)\n        e_ood   = energy(logits_ood)\n        gap     = (e_ood.mean() - e_id.mean()).item()\n        loss_id  = torch.clamp(e_id  - M_IN , min=0).pow(2).mean()\n        loss_ood = torch.clamp(M_OUT - e_ood, min=0).pow(2).mean()\n        loss_oe  = lam * (loss_ood + ALPHA_ID * loss_id)\n        loss     = loss_ce + loss_oe\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        run_gap += gap\n        correct += (logits_id.argmax(1) == y_id).sum().item()\n        total   += y_id.size(0)\n\n        if (i+1) % 100 == 0 or (i+1) == len(train_id_loader):\n            print(f\"batch {i+1:4d} | gap={gap:4.2f} | \"f\"CE={loss_ce.item():.3f} | OE={loss_oe.item():.3f}\")\n\n\n    train_acc = 100 * correct / total\n    print(f\"Train Acc {train_acc:.2f}% — mean gap {run_gap/len(train_id_loader):.2f}\")\n\n    # Validation \n    model.eval()\n    val_loss = val_correct = val_total = 0\n\n    val_loop = tqdm(\n        val_id_loader,\n        total=len(val_id_loader),\n        desc=f\"Val   {epoch+1}\",\n        leave=False\n    )\n\n    with torch.no_grad():\n        for x, y in val_loop:\n            out = model(x.to(device))\n            l = criterion_val(out, y.to(device)).item()\n            val_loss    += l\n            val_correct += (out.argmax(1) == y.to(device)).sum().item()\n            val_total   += y.size(0)\n            val_loop.set_postfix({\"val_loss\": f\"{val_loss/(val_loop.n+1):.4f}\", \n                                  \"val_acc\": f\"{100*val_correct/val_total:5.2f}%\"})\n\n    val_loss /= len(val_id_loader)\n    val_acc   = 100 * val_correct / val_total\n    print(f\"Val Loss {val_loss:.4f} — Acc {val_acc:.2f}%\")\n\n    # early-stop \n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        best_wts      = copy.deepcopy(model.state_dict())\n        torch.save(best_wts, \"best_model_weights.pth\")\n        no_improve = 0\n        print(\"Best model saved.\")\n    else:\n        no_improve += 1\n        print(f\"⏳ no improvement ({no_improve}/{PATIENCE})\")\n        if no_improve >= PATIENCE:\n            print(\"Early stop triggered.\")\n            break\n\n    # scheduler \n    scheduler.step()\n\n# Re-calibration BN \nmodel.load_state_dict(best_wts)\nmodel.train()\nwith torch.no_grad():\n    for x, _ in DataLoader(train_id, batch_size=256, shuffle=False):\n        _ = model(x.to(device))\nmodel.eval()\nprint(\"Best weights loaded & BN re-calibrated.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation\n\nAfter training, we assess both classification accuracy on Food-101 (ID) and out-of-distribution performance on SVHN (OOD). First, we switch the model to evaluation mode and run a simple test loop over held-out Food-101 images to report accuracy.\n\nNext, we compute two OOD detection scores for each image: energy and maximum softmax probability (MSP). Both scores are computed using temperature scaling, by dividing logits by a fixed temperature T before applying softmax (for MSP) or computing the energy score. This calibration step is crucial for obtaining meaningful and separable confidence values across ID and OOD samples.\n","metadata":{}},{"cell_type":"code","source":"T = 0.35                      \n\nmodel.eval()\n\ncorrect = 0\ntotal = 0\n\n# Accuracy\n\nwith torch.no_grad():\n    test_loop = tqdm(test_loader, total=len(test_loader), desc=\"Testing\")\n    for images, labels in test_loop:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint(f\"\\nFinal Test Accuracy: {100 * correct / total:.2f}%\")\n\n# Scoring function\n\n@torch.no_grad()\ndef get_scores(loader):\n    energy, msp = [], []\n    for batch in tqdm(loader, desc=\"Scoring\", leave=False):\n        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n        x = x.to(device, non_blocking=True)\n\n        logits = model(x)\n        energy.append((-torch.logsumexp(logits / T, 1)).cpu())          # Energy\n        msp.append(torch.softmax(logits, 1).max(1).values.cpu())        # MSP\n    return torch.cat(energy).numpy(), torch.cat(msp).numpy()\n\n\nid_energy,  id_msp  = get_scores(test_loader)      # Food-101 \nood_energy, ood_msp = get_scores(ood_loader)   # SVHN\n\n\n# AUROC / AUPR  \n\nlabels = np.concatenate([np.ones_like(id_energy),\n                         np.zeros_like(ood_energy)])\n\nauroc_e = roc_auc_score(labels, -np.concatenate([id_energy, ood_energy]))\nauroc_m = roc_auc_score(labels,  np.concatenate([id_msp,   ood_msp  ]))\n\npr_e, rc_e, _ = precision_recall_curve(labels, -np.concatenate([id_energy, ood_energy]))\npr_m, rc_m, _ = precision_recall_curve(labels,  np.concatenate([id_msp,    ood_msp ]))\naupr_e = average_precision_score(labels, -np.concatenate([id_energy, ood_energy]))\naupr_m = average_precision_score(labels,  np.concatenate([id_msp,    ood_msp ]))\n\n\n# False-positive rate \n\nfpr_e, tpr_e, _ = roc_curve(labels, -np.concatenate([id_energy, ood_energy]))\nfpr_m, tpr_m, _ = roc_curve(labels,  np.concatenate([id_msp,   ood_msp  ]))\n\ndef fpr_at_tpr(fpr, tpr, target=0.95):\n    return np.interp(target, tpr, fpr) if tpr[-1] >= target else 1.0\n\nfpr95_e = fpr_at_tpr(fpr_e, tpr_e)\nfpr95_m = fpr_at_tpr(fpr_m, tpr_m)\n\n\n# Results\n\nprint(\"\\n  METRICHE OOD\")\nprint(f\"• AUROC  (Energy)      : {auroc_e:.4f}\")\nprint(f\"• AUROC  (Soft-max)    : {auroc_m:.4f}\")\nprint(f\"• FPR@95TPR (Energy)   : {fpr95_e*100:.2f}%\")\nprint(f\"• FPR@95TPR (Soft-max) : {fpr95_m*100:.2f}%\")\nprint(f\"• AUPR-In (Energy)     : {aupr_e:.4f}\")\nprint(f\"• AUPR-In (Soft-max)   : {aupr_m:.4f}\")\n\n\n# Curve ROC / PR\n\nplt.figure(figsize=(14,5))\n# ROC\nplt.subplot(1,2,1)\nplt.plot(fpr_e, tpr_e, label=f\"Energy (AUROC={auroc_e:.4f})\")\nplt.plot(fpr_m, tpr_m, label=f\"MSP   (AUROC={auroc_m:.4f})\")\nplt.plot([0,1],[0,1],'--',color='gray')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC — OOD Detection\"); plt.legend(); plt.grid(True)\n# PR\nplt.subplot(1,2,2)\nplt.plot(rc_e, pr_e, label=f\"Energy (AUPR={aupr_e:.4f})\")\nplt.plot(rc_m, pr_m, label=f\"MSP   (AUPR={aupr_m:.4f})\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall (ID positive)\"); plt.legend(); plt.grid(True)\nplt.tight_layout(); plt.show()\n\n\n# Histograms\n\nfig, ax = plt.subplots(1,2, figsize=(14,4))\nax[0].hist(id_energy,  bins=100, alpha=0.6, label=\"ID (Food-101)\")\nax[0].hist(ood_energy, bins=100, alpha=0.6, label=\"OOD (SVHN)\")\nax[0].set_title(\"Distribuzione Energy\"); ax[0].legend()\n\nax[1].hist(id_msp,  bins=100, alpha=0.6, label=\"ID (Food-101)\")\nax[1].hist(ood_msp, bins=100, alpha=0.6, label=\"OOD (SVHN)\")\nax[1].set_title(\"Distribuzione MSP\");  ax[1].legend()\nplt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation with Flower Dataset\nWe use the `train/` split of the Flower Dataset as our OOD test set to assess how well the model generalizes to previously unseen visual domains. Although the model was trained only to classify food categories from the Food-101 dataset, a robust OOD detector should also flag entirely unrelated categories, such as flowers, as unfamiliar.","metadata":{}},{"cell_type":"code","source":"T = 0.2    \n\nmodel.eval()\n\n# Scoring function\n\n@torch.no_grad()\ndef get_scores(loader):\n    energy, msp = [], []\n    for batch in tqdm(loader, desc=\"Scoring\", leave=False):\n        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n        x = x.to(device, non_blocking=True)\n\n        logits = model(x)\n        energy.append((-torch.logsumexp(logits / T, 1)).cpu())          # Energy\n        msp.append(torch.softmax(logits, 1).max(1).values.cpu())        # MSP\n    return torch.cat(energy).numpy(), torch.cat(msp).numpy()\n\n\n# Calcolo degli score\nid_energy,  id_msp  = get_scores(test_loader)     # Food-101 \nood_energy, ood_msp = get_scores(flower_ood_loader)      # Tiny ImageNet \n\n\n# AUROC / AUPR  \nlabels = np.concatenate([np.ones_like(id_energy),\n                         np.zeros_like(ood_energy)])\n\nauroc_e = roc_auc_score(labels, -np.concatenate([id_energy, ood_energy]))\nauroc_m = roc_auc_score(labels,  np.concatenate([id_msp,   ood_msp  ]))\n\npr_e, rc_e, _ = precision_recall_curve(labels, -np.concatenate([id_energy, ood_energy]))\npr_m, rc_m, _ = precision_recall_curve(labels,  np.concatenate([id_msp,    ood_msp ]))\naupr_e = average_precision_score(labels, -np.concatenate([id_energy, ood_energy]))\naupr_m = average_precision_score(labels,  np.concatenate([id_msp,    ood_msp ]))\n\n\n# False-positive rate \n\nfpr_e, tpr_e, _ = roc_curve(labels, -np.concatenate([id_energy, ood_energy]))\nfpr_m, tpr_m, _ = roc_curve(labels,  np.concatenate([id_msp,   ood_msp  ]))\n\ndef fpr_at_tpr(fpr, tpr, target=0.95):\n    return np.interp(target, tpr, fpr) if tpr[-1] >= target else 1.0\n\nfpr95_e = fpr_at_tpr(fpr_e, tpr_e)\nfpr95_m = fpr_at_tpr(fpr_m, tpr_m)\n\n\n# Risultati\n\nprint(\"\\n  METRICHE OOD\")\nprint(f\"• AUROC  (Energy)      : {auroc_e:.4f}\")\nprint(f\"• AUROC  (Soft-max)    : {auroc_m:.4f}\")\nprint(f\"• FPR@95TPR (Energy)   : {fpr95_e*100:.2f}%\")\nprint(f\"• FPR@95TPR (Soft-max) : {fpr95_m*100:.2f}%\")\nprint(f\"• AUPR-In (Energy)     : {aupr_e:.4f}\")\nprint(f\"• AUPR-In (Soft-max)   : {aupr_m:.4f}\")\n\n\n# Curve ROC / PR\n\nplt.figure(figsize=(14,5))\n# ROC\nplt.subplot(1,2,1)\nplt.plot(fpr_e, tpr_e, label=f\"Energy (AUROC={auroc_e:.4f})\")\nplt.plot(fpr_m, tpr_m, label=f\"MSP   (AUROC={auroc_m:.4f})\")\nplt.plot([0,1],[0,1],'--',color='gray')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC — OOD Detection (Tiny ImageNet)\"); plt.legend(); plt.grid(True)\n\n# PR\nplt.subplot(1,2,2)\nplt.plot(rc_e, pr_e, label=f\"Energy (AUPR={aupr_e:.4f})\")\nplt.plot(rc_m, pr_m, label=f\"MSP   (AUPR={aupr_m:.4f})\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall (ID positive)\"); plt.legend(); plt.grid(True)\nplt.tight_layout(); plt.show()\n\n\n# Istogrammi\n\nfig, ax = plt.subplots(1,2, figsize=(14,4))\nax[0].hist(id_energy,  bins=100, alpha=0.6, label=\"ID (Food-101)\")\nax[0].hist(ood_energy, bins=100, alpha=0.6, label=\"OOD (Tiny ImageNet)\")\nax[0].set_title(\"Distribuzione Energy\"); ax[0].legend()\n\nax[1].hist(id_msp,  bins=100, alpha=0.6, label=\"ID (Food-101)\")\nax[1].hist(ood_msp, bins=100, alpha=0.6, label=\"OOD (Tiny ImageNet)\")\nax[1].set_title(\"Distribuzione MSP\");  ax[1].legend()\nplt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}